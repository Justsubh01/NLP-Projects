{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"color: #484848;font-size: 35px;font-weight: bold;padding:30px;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\">NLP Guide(Part 1): üìàEDA, text pre-processing &amp; Embeddings.</span></h1>\n\n***\n","metadata":{}},{"cell_type":"markdown","source":"<blockquote><center><h2 style=\"color: #484848;font-size: 25px;font-weight: bold;font-family: monospace;padding-bottom:30px\" >Introduction üìù</h2></center>\n    <p style=\"font-size:16px;  font-family:monospace;\">\n        The Goalü•Ö of this notebook is to explore text data in a clean beautiful manner.For this i am using \n        <a href='https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge',style=\"color:#159364; font-family:cursive;\">Toxic Comment Classification Challenge</a>\n        data.It is a multi-label classification problem, for detecting different types of toxicity like threats,obscenity,insults and identity-based hate.we will explore how imbalanced this dataset is,find correlations between labels and more.\nI will show you some basic  text preprocessing techniques and understand pre-trained embeddings. \n        \n   </p>\n   <br>\n  \n<h3 style=\"color: #484848;font-size: 15px;font-weight: bold;font-family: monospace;\" >Multi-label Classification:=</h3>\n    <p style=\"font-size:16px;  font-family:monospace;\">\n     In multi-label classification tasks, each problem instance is associated with multiple classes at the same time.Emotion identification, image annotation, text categorization, semantic scene classification, or gene and protein function prediction are examples of such problems.There are strategies to solve multi-label classification problems.In which the first is converts the multi-label problem into multiple single-label classification problems and ignores the correlation among labels,the second is to consider the pair-wise correlation between labels, and the third is to looks at the high order correlation through a subset of labels.\n    </p>\n\n<h2 style=\"color: #484848;font-size: 25px;font-weight: bold;font-family: monospace;padding-bottom:30px\" >Table of content</h2>\n  \n \n<ul style=\"font-size:16px;  font-family:monospace;\">\n    \n  <li><a href='#1'>Data Loading üìÇ</a></li>\n  <li><a href='#2'>EDAüìà</a></li>\n  <li><a href='#3'>WordClouds‚òÅÔ∏è</a></li>\n  <li><a href='#4'>Pre-processing‚öíÔ∏è</a></li>\n  <li><a href='#5'>Word vector</a></li>\n  <li><a href='#6'>Glove Embedding</a></li>\n  <li><a href='#7'>Model TrainingüèãÔ∏è</a></li>\n  <li><a href='#8'>References</a></li>\n  \n    \n</ul>\n</blockquote>","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"color:red;; font-family:newtimeroman; font-size:80%;padding:100px; text-align:center; \">Disclaimer: the dataset used in this notebook contains text that may be considered profane, vulgar, or offensive.</p>","metadata":{"execution":{"iopub.status.busy":"2021-06-06T12:42:11.167388Z","iopub.execute_input":"2021-06-06T12:42:11.167762Z","iopub.status.idle":"2021-06-06T12:42:11.172145Z","shell.execute_reply.started":"2021-06-06T12:42:11.167722Z","shell.execute_reply":"2021-06-06T12:42:11.171192Z"}}},{"cell_type":"code","source":"import sys\nimport random\nimport operator\nimport zipfile\nfrom time import sleep\nimport re\nimport gc\nimport math\nimport os\nimport time\nimport string\nimport random\nimport pickle\n\nimport pandas as pd \nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom plotly import graph_objs as go\nfrom IPython.display import display_html,HTML, Javascript\n\nimport tensorflow as tf \nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\nfrom plotly.subplots import make_subplots\n\nfrom PIL import Image\n\nfrom tqdm.autonotebook import tqdm\n\n\n\n\nimport nltk\nfrom pandas import DataFrame\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer \nfrom sklearn.feature_extraction.text import CountVectorizer as CV\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer \n# from cuml.feature_extraction.text import CountVectorizer\n# from cuml.neighbors import NearestNeighbors\nfrom sklearn.model_selection import train_test_split\nfrom colorama import Fore, Back, Style\nfrom wordcloud import WordCloud,STOPWORDS\nfrom PIL import Image\nfrom nltk.tokenize import word_tokenize\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model,load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.preprocessing import text, sequence\n\nnltk.download('stopwords')\n\nfrom pandas_profiling import ProfileReport\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom pprint import pprint\n\n\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-09T06:57:13.261947Z","iopub.execute_input":"2021-06-09T06:57:13.262381Z","iopub.status.idle":"2021-06-09T06:57:24.976756Z","shell.execute_reply.started":"2021-06-09T06:57:13.262295Z","shell.execute_reply":"2021-06-09T06:57:24.975642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining all our palette colours.\nprimary_azure = \"#8A2BE2\"\nprimary_slate = \"#CCFFFF\"\nprimary_aquamarine = \"#7FFFD4\"\nprimary_crimson = '#1E90FF'\nprimary_venom_green = \"#728C00\"\nprimary_orchid = \"#D8BFD8\"\nprimary_blue = \"#496595\"\nprimary_blue2 = \"#85a1c1\"\nprimary_blue3 = \"#3f4d63\"\n\nprimary_green = px.colors.qualitative.Plotly[2]\n\n# Defining all colors for text.\nHEAD = '\\x1b[1;31;47m'\nCOMMENT1='\\x1b[1;30;43m'\nCOMMENT2='\\x1b[1;30;44m'\n\nsns.set(rc={'axes.facecolor':'white', 'figure.facecolor':'#234E70'})\npd.set_option('display.max_columns',10)\nsys.setrecursionlimit(10**8)\n\nBASE_PATH = '../input/jigsaw-toxic-comment-classification-challenge'\nunzip = zipfile.ZipFile('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\nunzip.extractall()\nunzip = zipfile.ZipFile('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\nunzip.extractall()\nunzip = zipfile.ZipFile('../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip')\nunzip.extractall()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-09T06:57:37.177181Z","iopub.execute_input":"2021-06-09T06:57:37.17761Z","iopub.status.idle":"2021-06-09T06:57:40.112419Z","shell.execute_reply.started":"2021-06-09T06:57:37.17757Z","shell.execute_reply":"2021-06-09T06:57:40.111613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='1'></a>\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:cursive;\">\n    <h1 style=\"color: #484848;font-size: 35px;font-weight: bold;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\"><center>Data LoadingüìÇ</center></span></h1>\n</div>","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('./train.csv')\ntest_df = pd.read_csv('./test.csv')\ntrain_df.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-09T06:57:40.669597Z","iopub.execute_input":"2021-06-09T06:57:40.670146Z","iopub.status.idle":"2021-06-09T06:57:42.58378Z","shell.execute_reply.started":"2021-06-09T06:57:40.670107Z","shell.execute_reply":"2021-06-09T06:57:42.582683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['text_len'] = train_df['comment_text'].apply(lambda x: len(x.split(' ')))\ntest_df['text_len'] = test_df['comment_text'].apply(lambda x: len(x.split(' ')))\n\ncolors = [primary_slate,primary_aquamarine,primary_azure,primary_crimson,primary_venom_green,primary_green]\nlabel_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ntrain_df['none'] = 1-train_df[label_cols].max(axis=1)\nlabel_with_clean = label_cols.copy()\nlabel_with_clean.append('none')\n\ntrain_df['comment_text'].fillna(\"unknown\", inplace=True)\ntest_df['comment_text'].fillna(\"unknown\", inplace=True)\nprint(HEAD+\"train data \\n\"+COMMENT1+\"total data entries: \\t\"+COMMENT2+f\"{len(train_df)}\"+COMMENT1 +\"\\nmaximum comment len: \\t\"\\\n      +COMMENT2 +f\"{max(train_df['text_len'])}\"+COMMENT1+\"\\nminimum comment len: \\t\"+COMMENT2+f\"{min(train_df['text_len'])}\"+\\\n      COMMENT1 +\"\\nmean comment len: \\t\"+COMMENT2 +f\"{train_df['text_len'].mean()}\\n\")\n\nprint(HEAD+f\"test data \\n\"+COMMENT1+\"total data entries: \\t\"+COMMENT2+f\"{len(test_df)}\"+COMMENT1 +\"\\nmaximum comment len: \\t\" \\\n      +COMMENT2 +f\"{max(test_df['text_len'])}\"+ COMMENT1 +\"\\nminimum comment len: \\t\"+COMMENT2+f\"{min(test_df['text_len'])}\"+ COMMENT1 +\"\\nmean comment len: \\t\"+ \\\n      COMMENT2 +f\"{test_df['text_len'].mean()}\")\n\nprint(HEAD+f\"\\nClean Vs. Toxic \\n\"+COMMENT1+\"clean comments: \\t\"+COMMENT2+f\"{len(train_df[train_df.none==1])}\" \\\n     +COMMENT1+\"\\nAll kind of toxic comments: \\t\"+COMMENT2+f\"{len(train_df[train_df.none==0])}\")\n\nprint(HEAD+f\"\\nAll kind of toxicity \\n\"+COMMENT1+\"toxic: \\t\\t\"+COMMENT2+f\"{len(train_df[train_df.toxic==1])}\"\\\n     +COMMENT1+f\"\\nsevere toxic: \\t\"+COMMENT2+f\"{len(train_df[train_df.severe_toxic==1])}\"\\\n     +COMMENT1+f\"\\nobscene: \\t\"+COMMENT2+f\"{len(train_df[train_df.obscene==1])}\"\\\n     +COMMENT1+f\"\\nthreat: \\t\"+COMMENT2+f\"{len(train_df[train_df.threat==1])}\"\\\n     +COMMENT1+f\"\\ninsult: \\t\"+COMMENT2+f\"{len(train_df[train_df.insult==1])}\"\\\n     +COMMENT1+f\"\\nidentity hate: \\t\"+COMMENT2+f\"{len(train_df[train_df.identity_hate==1])}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-09T06:57:42.58567Z","iopub.execute_input":"2021-06-09T06:57:42.586106Z","iopub.status.idle":"2021-06-09T06:57:44.160721Z","shell.execute_reply.started":"2021-06-09T06:57:42.586059Z","shell.execute_reply":"2021-06-09T06:57:44.159546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2'></a>\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:cursive;\">\n    <h1 style=\"color: #484848;font-size: 35px;font-weight: bold;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\"><center>EDAüìà</center></span></h1>\n</div>\n\n<p style=\"font-size:16px;  font-family:monospace;\">\nThere is not a lot of difference between size of train and test data.but data is highly imbalanced,almost 90% comments are clean and only 10% comments have some kind of toxicity.\nlooking at the insights above we can say that the comments are holding multiple tags, we will see insights about this but first let's check some comments of all tags and try to determine how they categorized.\n</p>","metadata":{}},{"cell_type":"code","source":"print(HEAD+\"clean comment:\\n\")\nprint(COMMENT1+train_df[train_df.none==1].iloc[3,1])","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-06-09T06:57:44.162671Z","iopub.execute_input":"2021-06-09T06:57:44.16297Z","iopub.status.idle":"2021-06-09T06:57:44.185659Z","shell.execute_reply.started":"2021-06-09T06:57:44.162941Z","shell.execute_reply":"2021-06-09T06:57:44.184595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(HEAD+\"toxic comment:\\n\")\nprint(COMMENT1+train_df[train_df.toxic==1].iloc[3,1])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-09T06:57:44.232649Z","iopub.execute_input":"2021-06-09T06:57:44.233021Z","iopub.status.idle":"2021-06-09T06:57:44.244701Z","shell.execute_reply.started":"2021-06-09T06:57:44.232986Z","shell.execute_reply":"2021-06-09T06:57:44.243491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(HEAD+'severe_toxic:\\n')\nprint(COMMENT1+train_df[train_df.severe_toxic==1].iloc[2,1])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-08T22:01:58.199803Z","iopub.execute_input":"2021-06-08T22:01:58.20049Z","iopub.status.idle":"2021-06-08T22:01:58.211186Z","shell.execute_reply.started":"2021-06-08T22:01:58.200446Z","shell.execute_reply":"2021-06-08T22:01:58.209435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(HEAD+'threat:\\n')\nprint(COMMENT1+train_df[train_df.threat==1].iloc[7,1])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-08T22:01:58.213667Z","iopub.execute_input":"2021-06-08T22:01:58.214272Z","iopub.status.idle":"2021-06-08T22:01:58.226255Z","shell.execute_reply.started":"2021-06-08T22:01:58.214205Z","shell.execute_reply":"2021-06-08T22:01:58.224342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(HEAD+'insult:\\n')\nprint(COMMENT1+train_df[train_df.insult==1].iloc[22,1])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-08T22:01:58.228669Z","iopub.execute_input":"2021-06-08T22:01:58.229279Z","iopub.status.idle":"2021-06-08T22:01:58.244184Z","shell.execute_reply.started":"2021-06-08T22:01:58.229219Z","shell.execute_reply":"2021-06-08T22:01:58.24278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(HEAD+\"Obscene:\\n\")\nprint(COMMENT1+train_df[train_df.obscene==1].iloc[2,1])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-08T22:01:58.844602Z","iopub.execute_input":"2021-06-08T22:01:58.845074Z","iopub.status.idle":"2021-06-08T22:01:58.858278Z","shell.execute_reply.started":"2021-06-08T22:01:58.845043Z","shell.execute_reply":"2021-06-08T22:01:58.856736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(HEAD+\"identity_hate:\\n\")\nprint(COMMENT1+train_df[train_df.identity_hate==1].iloc[3,1])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-08T22:01:59.726198Z","iopub.execute_input":"2021-06-08T22:01:59.726591Z","iopub.status.idle":"2021-06-08T22:01:59.737991Z","shell.execute_reply.started":"2021-06-08T22:01:59.726562Z","shell.execute_reply":"2021-06-08T22:01:59.736209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"balance_counts = train_df[label_with_clean].sum().tolist()\nbalance_sum = pd.DataFrame(balance_counts, columns=['toxicity'],index=label_with_clean)\nbalance_sum.drop(['none'],inplace=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-08T22:02:00.038566Z","iopub.execute_input":"2021-06-08T22:02:00.039054Z","iopub.status.idle":"2021-06-08T22:02:00.069533Z","shell.execute_reply.started":"2021-06-08T22:02:00.039021Z","shell.execute_reply":"2021-06-08T22:02:00.067932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure()\nfor i,label in enumerate(zip(label_with_clean,colors)):\n    fig.add_trace(go.Bar(\n    x=[label[0]],\n    y=[balance_counts[i]],\n    name=label[0],\n    text=[balance_counts[i]],\n    textposition='auto',\n    marker_color=label[1]\n    ))\nfig.update_layout(\n    paper_bgcolor='#234E70',\n    plot_bgcolor='rgba(0,0,0,0)',\n    title='<span style=\"font-size:32px; font-family:Times New Roman\">Dataset distribution by labels</span>',\n    font=dict(color='white',family=\"Courier New, monospace\"),\n)\nfig.show()\n\n\nfig1 = px.pie(train_df['none'].value_counts().reset_index(), values='none',names=['non-toxic',\"All type of toxicity\"])\nfig1.update_traces(textposition=\"inside\", textinfo='percent+label')\nfig1.update_layout(paper_bgcolor='#234E70',\n    plot_bgcolor='rgba(0,0,0,0)',\n    font=dict(color='white',family=\"Courier New, monospace\"),\n    title='<span style=\"font-size:32px; font-family:Times New Roman\">Distribution by very bad amd normal comments</span>',)\nfig1.show()\n\nfig1 = px.pie(balance_sum,values='toxicity',names=[\"toxic\",'severe_toxic','obscene',\"threat\",\"insult\",\"identity_hate\"])\nfig1.update_traces(textposition=\"inside\", textinfo=\"percent+label\")\nfig1.update_layout(paper_bgcolor='#234E70',\n    plot_bgcolor='rgba(0,0,0,0)',\n    font=dict(color='white',family=\"Courier New, monospace\"),\n    title='<span style=\"font-size:32px; font-family:Times New Roman\">Distribution between types of toxic comments</span>',)\nfig1.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-08T22:02:01.258822Z","iopub.execute_input":"2021-06-08T22:02:01.25927Z","iopub.status.idle":"2021-06-08T22:02:02.26861Z","shell.execute_reply.started":"2021-06-08T22:02:01.259239Z","shell.execute_reply":"2021-06-08T22:02:02.26692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"row_sums = train_df[label_cols].sum(axis=1)\nx = row_sums.value_counts()\n\n\nfig = go.Figure()\nfor i,label in enumerate(x):\n    fig.add_trace(go.Bar(\n        x=[i],\n        y=[label],\n        name=i,\n        text=[label],\n        textposition='auto',\n        marker_color=[colors[5]]\n    ))\nfig.update_layout(\n    paper_bgcolor='#234E70',\n    plot_bgcolor='rgba(0,0,0,0)',\n    title='<span style=\"font-size:32px; font-family:Times New Roman\">Comments with multiple labels</span>',\n    font=dict(color='white',family=\"Courier New, monospace\"),\n)\nfig.show()    ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-08T22:02:02.510952Z","iopub.execute_input":"2021-06-08T22:02:02.511386Z","iopub.status.idle":"2021-06-08T22:02:02.552024Z","shell.execute_reply.started":"2021-06-08T22:02:02.511356Z","shell.execute_reply":"2021-06-08T22:02:02.550925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote>\n<h3 style=\"color: #484848;font-size: 20px;font-weight: bold;font-family: monospace;\" >Correlation between lab:=</h3>\n<p style=\"font-size:16px;  font-family:monospace;\">\nlooking at the chart above we can see multiple entries are holding multiple tags and that is the normal behavior of multilabel classification problems because of that tags have correlation with other tags,for instance a comment can be type of toxic,severe-toxic and obscene.lets visualize correlation between labels.\n</p>\n</blockquote>","metadata":{}},{"cell_type":"code","source":"def cutout(seq, idx):\n    return seq[idx+1:] + seq[:idx]\n\ncorr_df = []\nfor i,main_cols in enumerate(zip(train_df.columns[2:8],colors)):\n    corr_mats = []\n    for other_cols in cutout(label_cols,i):\n        confusion_matrix = pd.crosstab(train_df[main_cols[0]],train_df[other_cols])\n        corr_mats.append(confusion_matrix)\n    out = pd.concat(corr_mats,axis=1,keys=cutout(label_cols,i))\n    out = out.style.highlight_min(color=colors[i], axis=0)\n    out = out.set_table_styles([{'selector':'th','props':[('background-color','#234E70'),('color', 'cyan')]}])\n    corr_df.append(out)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-08T22:02:06.582372Z","iopub.execute_input":"2021-06-08T22:02:06.582727Z","iopub.status.idle":"2021-06-08T22:02:07.278063Z","shell.execute_reply.started":"2021-06-08T22:02:06.582673Z","shell.execute_reply":"2021-06-08T22:02:07.276982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1_styler = corr_df[0].set_table_attributes(\"style='display:inline;margin:40px;font-family:monospace;font-style: oblique;font-weight: bold;font-variant: small-caps;font-size: 16px;color:black;'\").set_caption('Correlation between toxic and other labels')\ndf2_styler = corr_df[1].set_table_attributes(\"style='display:inline;margin:40px;font-family:monospace;font-style: oblique;font-weight: bold;font-variant: small-caps;font-size: 16px;color:black;'\").set_caption('Correlation between severe-toxic and other labels')\ndf3_styler = corr_df[2].set_table_attributes(\"style='display:inline;margin:40px;font-family:monospace;font-style: oblique;font-weight: bold;font-variant: small-caps;font-size: 17px;color:black;'\").set_caption('Correlation between obscene and other labels')\ndf4_styler = corr_df[3].set_table_attributes(\"style='display:inline;margin:40px;font-family:monospace;font-style: oblique;font-weight: bold;font-variant: small-caps;font-size: 17px;color:black;'\").set_caption('Correlation between threat and other labels')\ndf5_styler = corr_df[4].set_table_attributes(\"style='display:inline;margin:40px;font-family:monospace;font-style: oblique;font-weight: bold;font-variant: small-caps;font-size: 16px;color:black;'\").set_caption('Correlation between insult and other labels')\ndf6_styler = corr_df[5].set_table_attributes(\"style='display:inline;margin:40px;font-family:monospace;font-style: oblique;font-weight: bold;font-variant: small-caps;font-size: 16px;color:black;'\").set_caption('Correlation between idendity-hate and other labels')\n\ndisplay_html(df1_styler._repr_html_(), raw=True)\ndisplay_html(df2_styler._repr_html_(), raw=True)\ndisplay_html(df3_styler._repr_html_(), raw=True)\ndisplay_html(df4_styler._repr_html_(), raw=True)\ndisplay_html(df5_styler._repr_html_(), raw=True)\ndisplay_html(df6_styler._repr_html_(), raw=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-08T22:02:12.464023Z","iopub.execute_input":"2021-06-08T22:02:12.464416Z","iopub.status.idle":"2021-06-08T22:02:12.97792Z","shell.execute_reply.started":"2021-06-08T22:02:12.464387Z","shell.execute_reply":"2021-06-08T22:02:12.976789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote>\n<h3 style=\"color: #484848;font-size: 20px;font-weight: bold;font-family: monospace;\" >Text data noises:=</h3>\n<p style=\"font-size:16px;  font-family:monospace;\">\nText data contains many kind of impurities and noises that is build false correlations between labels.How to deal with these impurities totally depends on our approach to solve the problem, i will show you two approachs to deal with theses impurities in this notebook,but first we visualize thses noises.they are just punctuations('$',\"'\",',','&') ,urls, stopwords('the','i','are','and') words that do not give much insights about data and common in all instances.\n</p>\n</blockquote>","metadata":{"execution":{"iopub.status.busy":"2021-06-07T09:36:28.881844Z","iopub.execute_input":"2021-06-07T09:36:28.88229Z","iopub.status.idle":"2021-06-07T09:36:28.888582Z","shell.execute_reply.started":"2021-06-07T09:36:28.882253Z","shell.execute_reply":"2021-06-07T09:36:28.887535Z"}}},{"cell_type":"code","source":"stop_words = stopwords.words('english')\nmore_stop_words = ['u','im','c','ur']\nstop_words = stop_words + more_stop_words\n\n#unique_word_count \ntrain_df[\"unique_word\"] = train_df[\"comment_text\"].apply(lambda x:len(set(str(x).split())))\ntest_df[\"unique_word\"] = train_df[\"comment_text\"].apply(lambda x:len(set(str(x).split())))\n\n# stop_word_count\ntrain_df[\"stop_word\"] = train_df['comment_text'].apply(lambda x:len([w for w in str(x).lower().split() if w in stop_words]))\ntest_df[\"stop_word\"] = test_df['comment_text'].apply(lambda x:len([w for w in str(x).lower().split() if w in stop_words]))\n\n# url_count\ntrain_df['url_count'] = train_df['comment_text'].apply(lambda x:len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\ntest_df['url_count'] = test_df['comment_text'].apply(lambda x:len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n\n# char_count\ntrain_df['char_count'] = train_df['comment_text'].apply(lambda x: len(str(x)))\ntest_df['char_count'] = test_df['comment_text'].apply(lambda x: len(str(x)))\n\n# punctuation_count\ntrain_df['punctuation_count'] = train_df['comment_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ntest_df['punctuation_count'] = test_df['comment_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-08T22:02:15.159488Z","iopub.execute_input":"2021-06-08T22:02:15.160175Z","iopub.status.idle":"2021-06-08T22:03:04.205889Z","shell.execute_reply.started":"2021-06-08T22:02:15.1601Z","shell.execute_reply":"2021-06-08T22:03:04.204774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nbalance_counts = train_df[labels_cols].sum().tolist()\n\ndef nano_method(num):\n    if num%2==0:return 2\n    else:return 1\n    \ndfs={}\nfor label in labels_cols:\n    dfs[label] = train_df[train_df[label]==1]\n\nfig = make_subplots(rows=3,cols=2)\n\nfor i, label in enumerate(labels_cols,1):\n        fig.add_trace(\n            go.Scatter(x=dfs[label][[label,'text_len']].index,\n                       y=dfs[label][[label,'text_len']].values[:,1],name=label,fill='tozeroy'),\n            row=math.ceil(i/2),col=nano_method(i)\n        )\n\nfig.update_layout(height=800, width=800,\n    paper_bgcolor='#234E70',\n    plot_bgcolor='rgba(0,0,0,0)',\n    title='<span style=\"font-size:32px; font-family:Times New Roman\">Data Roles in Different Fields</span>',\n    font=dict(color='white',family=\"Courier New, monospace\"),\n    \n)\nfig.update_xaxes(range=[0,160000])\nfig.update_yaxes(range=[0,2500])\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-08T22:03:04.207905Z","iopub.execute_input":"2021-06-08T22:03:04.208393Z","iopub.status.idle":"2021-06-08T22:03:04.40562Z","shell.execute_reply.started":"2021-06-08T22:03:04.208338Z","shell.execute_reply":"2021-06-08T22:03:04.404521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"METAFEATURES = ['text_len','unique_word', 'stop_word','url_count','char_count','punctuation_count']\n\nfig, axes = plt.subplots(ncols=2, nrows=len(METAFEATURES), figsize=(25,40),dpi=100)\n\n\nfor i, feature in tqdm(enumerate(METAFEATURES),desc='loop'):\n    sns.histplot(dfs['toxic'][feature], label='toxic', ax=axes[i][0], color=colors[5],kde=True,common_norm=True)\n    sns.histplot(dfs['severe_toxic'][feature], label='severe_toxic',ax=axes[i][0], color=colors[4],kde=True,common_norm=True)\n    sns.histplot(dfs['obscene'][feature], label='obscene', ax=axes[i][0], color=colors[3],kde=True,common_norm=True)\n    sns.histplot(dfs['threat'][feature], label='threat', ax=axes[i][0], color=colors[2],kde=True,common_norm=True)\n    sns.histplot(dfs['insult'][feature], label='insult', ax=axes[i][0], color=colors[1],kde=True,common_norm=True)\n    sns.histplot(dfs['identity_hate'][feature], label='identity_hate', ax=axes[i][0], color=colors[0],kde=True,common_norm=True)\n\n    sns.histplot(train_df[feature], label=\"Training\", ax=axes[i][1],color=colors[2],kde=True,common_norm=True)\n    sns.histplot(test_df[feature], label='Test', ax=axes[i][1],color=colors[0],common_norm=True)\n    sleep(0.01)\n    for j in range(2):\n        axes[i][j].set_xlabel('')\n        axes[i][j].set_yscale('symlog', base=5,)\n        axes[i][j].tick_params(axis='x', labelsize=20,colors='w')\n        axes[i][j].tick_params(axis='y', labelsize=20,colors='w')\n        axes[i][j].legend()\n    \n    axes[i][0].set_title(f\"{feature} Target Distribution in Training Set\", fontsize=20,color='white')\n    axes[i][1].set_title(f\"{feature} Training & Test Set Distribution\", fontsize=20,color='white')\nplt.show()\n         ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-08T22:03:04.407422Z","iopub.execute_input":"2021-06-08T22:03:04.40816Z","iopub.status.idle":"2021-06-08T22:07:17.022492Z","shell.execute_reply.started":"2021-06-08T22:03:04.408118Z","shell.execute_reply":"2021-06-08T22:07:17.020879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='3'></a>\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:cursive;\">\n    <h1 style=\"color: #484848;font-size: 35px;font-weight: bold;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\"><center>WordClouds‚òÅÔ∏è</center></span></h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"stopword = set(STOPWORDS)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-08T22:07:17.024526Z","iopub.execute_input":"2021-06-08T22:07:17.024929Z","iopub.status.idle":"2021-06-08T22:07:17.030893Z","shell.execute_reply.started":"2021-06-08T22:07:17.02489Z","shell.execute_reply":"2021-06-08T22:07:17.029466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#clean comments\nclean_mask=np.array(Image.open(\"../input/logogo/Descarga gratis Mensaje.png\"))\nclean_mask=clean_mask[:,:,1]\n#wordcloud for clean comments\nsubset=train_df[train_df.none==True]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"#234E70\",max_words=2000,mask=clean_mask,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(20,10))\nplt.axis(\"off\")\nplt.title('Top words for clean comments', \n          fontsize=25,color='white')\nplt.imshow(wc.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T22:07:17.034107Z","iopub.execute_input":"2021-06-08T22:07:17.034578Z","iopub.status.idle":"2021-06-08T22:08:00.599783Z","shell.execute_reply.started":"2021-06-08T22:07:17.034528Z","shell.execute_reply":"2021-06-08T22:08:00.598416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#toxic\nplt.figure(figsize=(40,50))\nplt.subplot(321)\ntoxic_mask = np.array(Image.open('../input/mzzzzzz/Negative Grade.jpeg'))\n# toxic_mask = toxic_mask[:,:,1]\n# words cloud for toxic comments\ntext = dfs['toxic'].comment_text.values\nwc = WordCloud(background_color='black',max_words=500,mask=toxic_mask,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.title('Words frequented in Toxic Comments', fontsize=25,color='black')\nplt.axis('off')\nplt.imshow(wc)\n\n\n\n#severe_toxic\nplt.subplot(322)\nsevere_toxic = np.array(Image.open(\"../input/masks-for-masks/Cat What Murderous Black Cat With Knife Halloween Sticker.jpeg\"))\n# severe_toxic = severe_toxic[:,:,1]\n# wprds cloud for severe_toxic comments\ntext = dfs['severe_toxic'].comment_text.values\nwc = WordCloud(background_color='black', max_words=500, mask=severe_toxic, stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.axis('off')\nplt.title('Words frequented in severe toxic comments',fontsize=25,color='black')\nplt.imshow(wc.recolor(colormap='Paired_r', random_state=122), alpha=0.96)\n\n#obscene\nplt.subplot(323)\nobscene_mask = np.array(Image.open('../input/masks-for-masks/Image about cute in anime by socoki on We Heart It.jpeg'))\n# obscene_mask = obscene_mask[:,:,1]\n# words cloud for objscene comments\ntext = dfs['obscene'].comment_text.values\nwc = WordCloud(background_color='black', max_words=500, mask=obscene_mask,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.axis('off')\nplt.title('Words frequented in obscene commnets', fontsize=25,color='black')\nplt.imshow(wc.recolor(colormap='RdGy', random_state=215), alpha=0.96)\n\n#threat\nplt.subplot(324)\nthreat_mask = np.array(Image.open('../input/masks-for-masks/dinner Tosha C_ on ArtStation at https __www.artstation.jpeg'))\n# threat_mask = threat_mask[:,:,1]\n# words cloud for objscene comments\ntext = dfs['threat'].comment_text.values\nwc = WordCloud(background_color='black', max_words=500, mask=threat_mask,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.axis('off')\nplt.title('Words frequented in threat commnets',fontsize=25,color='black')\nplt.imshow(wc.recolor(colormap='rainbow', random_state=215), alpha=0.96)\n\n#insult\nplt.subplot(325)\ninsult_mask = np.array(Image.open('../input/mzzzzzz/Marshmello Dj Art Print by Shootme - X-Small.jpeg'))\n# insult_mask = insult_mask[:,:,1]\n# words cloud for objscene comments\ntext = dfs['insult'].comment_text.values\nwc = WordCloud(background_color='black', max_words=200, mask=insult_mask,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.axis('off')\nplt.title('Words frequented in insult commnets',fontsize=25,color='black')\nplt.imshow(wc.recolor(colormap='plasma', random_state=215), alpha=0.96)\n\n#identity hate\nplt.subplot(326)\nidentity_hate_mask = np.array(Image.open('../input/masks-for-masks/Bloodwraith A Fantasy World of Survival and Torment.jpeg'))\n# identity_hate_mask = identity_hate_mask[:,:,1]\n# words cloud for objscene comments\ntext = dfs['identity_hate'].comment_text.values\nwc = WordCloud(background_color='black', max_words=600, mask=identity_hate_mask,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.axis('off')\nplt.title('Words frequented in identity hate commnets', fontsize=25,color='black')\nplt.imshow(wc.recolor(random_state=125), alpha=0.96)\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-08T22:08:00.601913Z","iopub.execute_input":"2021-06-08T22:08:00.602323Z","iopub.status.idle":"2021-06-08T22:08:25.335412Z","shell.execute_reply.started":"2021-06-08T22:08:00.602272Z","shell.execute_reply":"2021-06-08T22:08:25.334412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='4'></a>\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:cursive;\">\n    <h1 style=\"color: #484848;font-size: 35px;font-weight: bold;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\"><center>Pre-processing‚öíÔ∏è</center></span></h1>\n</div>\n<br>\n<p style=\"font-size:16px;  font-family:monospace;\">\n    Preprocessing is one of the major steps when we are dealing with any kind of text models.During this stage we have to look at the distribution of our data, what techniques are needed and how deep we should clean. Few preprocessing steps are converting to lowercase, removing punctuation, removing stop words and stemming/lemmatization.\n    In this notebook we will be train our model on pre-trained embeddings,<span style='font-weight:bold'>and remember when we train model by pre-trained embedings we should not follow normal pre-processing steps</span> because of doing this we lost lots of useful imformations.we will see why is that when we learn about word embeddings in below cells.\n</p>","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"color: #484848;font-size: 25px;font-weight: bold;font-family: monospace;\" >Lowercase=></h2>\n<p style=\"font-size:16px;  font-family:monospace;\">\nProgramming languages consider textual data as sensitive, which means that <span style=\"font-weight:bold\">Different</span> is different from <span style=\"font-weight:bold\">different</span>. we humans know that those both belong to same token but due to the character encoding those are considered as different tokens. Converting to lowercase is a very mandatory preprocessing step.\n</p>\n\n<h2 style=\"color: #484848;font-size: 25px;font-weight: bold;font-family: monospace;\" >Punctuation=></h2>\n\n<p style=\"font-size:16px;  font-family:monospace;\">\n    Punctuation are the unnecessary symbols that are in our corpus documents, we should be little careful with what we are doing with this. There might be few problems such as U.S ‚Äî us ‚ÄúUnited Stated‚Äù being converted to ‚Äúus‚Äù after the preprocessing.\n </p>\n \n<h2 style=\"color: #484848;font-size: 25px;font-weight: bold;font-family: monospace;\" >Stopwords=></h2>\n\n<p style=\"font-size:16px;  font-family:monospace;\">\n    Stopwords are the most commonly occuring words which don't give any valuable insights.And removing them normally gives better results and improves computations.\n </p>\n \n <h2 style=\"color: #484848;font-size: 25px;font-weight: bold;font-family: monospace;\" >Lemmatisation=></h2>\n \n <p style=\"font-size:16px;  font-family:monospace;\">\n    Lemmatisation is a way to reduce the word to root synonym of a word. Unlike Stemming, Lemmatisation makes sure that the reduced word is again a dictionary word (word present in the same language). WordNetLemmatizer can be used to lemmatize any word.\n </p>\n \n <h2 style=\"color: #484848;font-size: 25px;font-weight: bold;font-family: monospace;\" >Stemming=></h2>\n \n <p style=\"font-size:16px;  font-family:monospace;\">\n    Stemming reduces the word to its stem, meaning that playing and played are the same words which basically indicate an action play, so we convert playing and played to play. <span style=\"font-weight:bold\">it is very importent normal pre-processing but equally bad for pre-processing for glove embedding. </span>\n </p>","metadata":{}},{"cell_type":"code","source":"def remove_punctuation(text):\n    punclist = string.punctuation + string.digits\n    # remove punctuation from text\n    table_ = str.maketrans(punclist,' '*len(punclist))\n    text_list = ' '.join(text.translate(table_).split())\n    return text_list\n\ndef remove_stopwords(text):\n    stop_words = stopwords.words('english')\n    more_stopwords = ['u','ya','yo','imma','im','c','da','be','s','on','off','na','nah','noo']\n    stop_words = stop_words + more_stopwords\n    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n    return text\n\ndef remove_apostrophe(text):\n    return text.replace(\"'\",\"\")\n    \ndef stem_word(text):\n    stemmer = PorterStemmer()\n    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n    return text\n\ndef lemm_word(text):\n    lemmer = WordNetLemmatizer()\n    text = ' '.join(lemmer.lemmatize(word) for word in text.split(' '))\n    return text","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-09T06:57:53.681888Z","iopub.execute_input":"2021-06-09T06:57:53.682382Z","iopub.status.idle":"2021-06-09T06:57:53.694144Z","shell.execute_reply.started":"2021-06-09T06:57:53.682335Z","shell.execute_reply":"2021-06-09T06:57:53.692921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def standard_preprocess(text):\n    text = str(text).lower()\n    text = remove_punctuation(text)\n    text = remove_stopwords(text)\n    text = remove_apostrophe(text)\n    text = lemm_word(text)\n    text = stem_word(text)\n    text = remove_punctuation(text)\n    return text","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-08T22:08:39.095317Z","iopub.execute_input":"2021-06-08T22:08:39.095798Z","iopub.status.idle":"2021-06-08T22:08:39.104605Z","shell.execute_reply.started":"2021-06-08T22:08:39.09576Z","shell.execute_reply":"2021-06-08T22:08:39.102059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntqdm.pandas()\ntrain_df['simple_preprocess'] = train_df['comment_text'].progress_apply(standard_preprocess)\ntest_df['simple_preprocess'] = test_df['comment_text'].progress_apply(standard_preprocess)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T22:08:41.176296Z","iopub.execute_input":"2021-06-08T22:08:41.176666Z","iopub.status.idle":"2021-06-08T22:17:48.952258Z","shell.execute_reply.started":"2021-06-08T22:08:41.176636Z","shell.execute_reply":"2021-06-08T22:17:48.950956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5'></a>\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:cursive;\">\n    <h1 style=\"color: #484848;font-size: 35px;font-weight: bold;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\"><center>Word vector</center></span></h1>\n</div>\n<br>\n\n<p style=\"font-size:16px;  font-family:monospace;\">\n    Word vectors represent a significant leap forward in advancing our ability to analyze relationships across words, sentences, and documents. In doing so, they advance technology by providing machines with much more information about words than has previously been possible using traditional representations of words. It is word vectors that make technologies such as speech recognition and machine translation possible.\n <br>  \n   &nbsp&nbsp&nbsp&nbsp&nbsp word vectors represent words as multidimensional continuous floating point numbers where semantically similar words are mapped to proximate points in geometric space. In simpler terms, a word vector is a row of real-valued numbers (as opposed to dummy numbers) where each point captures a dimension of the word's meaning and where semantically similar words have similar vectors. This means that words such as wheel and engine should have similar word vectors to the word car (because of the similarity of their meanings), whereas the word banana should be quite distant. example here is showing that by using word vectors we can determine that:\n     <br>\n        <br>\n   &nbsp&nbsp&nbsp&nbsp&nbsp<span style=\"font-weight:bold;\">king - man + woman = queen</span>\n\n </p>\n","metadata":{}},{"cell_type":"markdown","source":"<a id='6'></a>\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:cursive;\">\n    <h1 style=\"color: #484848;font-size: 35px;font-weight: bold;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\"><center>Glove embedding</center></span></h1>\n</div>\n<br>\n\n<p style=\"font-size:16px;  font-family:monospace;\">\n    GloVe method is built on an important idea,\n    <br>\n    <br>\n    <blockquote>\n        <span style=\"color: #484848;font-size: 20px;font-weight: 50px;font-family: monospace; letter-spacing: 2px;cursor: pointer\">\n           \"You can derive semantic relationships between words from the co-occurrence matrix\"\n         </span>\n     </blockquote>\n    <br>\n    <br>\n    </p>\n    <p style=\"font-size:16px;  font-family:monospace;\">\n    Word embeddings are basically a form of word representation that bridges the human understanding of language to that of a machine. They have learned representations of text in an n-dimensional space where words that have the same meaning have a similar representation. Meaning that two similar words are represented by almost similar vectors that are very closely placed in a vector space.\n    <br>\n    &nbsp&nbsp&nbsp&nbsp&nbspGiven a corpus having V words, the co-occurrence matrix X will be a V x V   matrix, where the i th row and j th column of X, X_ij denotes how many times word i has co-occurred with word j. An example co-occurrence matrix might look as follows.\n </p>\n <br>\n    <img src=\"https://miro.medium.com/max/347/1*QWcK8CIDs8kMkOwsOxvywA.png\",width=\"500\" height=\"600\" class=\"center\" style=\"margin-left: auto;\n  margin-right: auto;align:center\">\n  \n <p style=\"font-size:12px; font-family:monospace;text-align:center;\">The co-occurrence matrix for the sentence ‚Äúthe cat sat on the mat‚Äù with a window size of 1. As you probably noticed it is a symmetric matrix.</p>","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"color: #484848;font-size: 20px;font-weight: bold;padding:30px;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\">How to preprocess text-data for Glove embeddings?</span></h1>\n\n<p style=\"font-size:16px;  font-family:monospace;\">\n    When we train our model by glove embeddings we should not follow usual apparoach for pre-processing because by doing that we can lost useful informations. target should be<span style='font-weight:bold'> get our vocabulary as close to the embeddings as possible.</span>Getting your vocabulary close to the pretrained embeddings means, that you should aim for your preprocessing,to result in tokens that are mostly covered by word vectors. \n    <br>\n    \"Each pretrained embedding needs its own preprocessing\"\n    <br>\n    If people used different preprocessing for training their embeddings you would also need to do the same,Especially point to can be quite challenging, if you want to concatenate embeddings as in this kernel. Imagine Embedding A preprocesses \"don't\" to a single token[\"dont\"] and Embedding B to two tokens[\"do\",\"n't\"]. You are basically not able to do both. So you need to find a compromise.\n    </p>","metadata":{}},{"cell_type":"code","source":"# these function are borrowed from:-\"https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda\"\n\nimport operator \n\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x\n\ndef build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-09T06:57:57.457713Z","iopub.execute_input":"2021-06-09T06:57:57.458065Z","iopub.status.idle":"2021-06-09T06:57:57.468604Z","shell.execute_reply.started":"2021-06-09T06:57:57.458034Z","shell.execute_reply":"2021-06-09T06:57:57.467677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CRAWL_EMBEDDING_PATH = '../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl'\nGLOVE_EMBEDDING_PATH = '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl'","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-09T06:57:58.827113Z","iopub.execute_input":"2021-06-09T06:57:58.827445Z","iopub.status.idle":"2021-06-09T06:57:58.832378Z","shell.execute_reply.started":"2021-06-09T06:57:58.827414Z","shell.execute_reply":"2021-06-09T06:57:58.831048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load embeddings\ntic = time.time()\nglove_embeddings = load_embeddings(GLOVE_EMBEDDING_PATH)\nprint(HEAD+f'loaded {len(glove_embeddings)} word vectors in {time.time()-tic}s')","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:58:02.573308Z","iopub.execute_input":"2021-06-09T06:58:02.573681Z","iopub.status.idle":"2021-06-09T06:58:36.46471Z","shell.execute_reply.started":"2021-06-09T06:58:02.573648Z","shell.execute_reply":"2021-06-09T06:58:36.463574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote><h1 style=\"color: #484848;font-size: 20px;font-weight: bold;padding:30px;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\">Check coverage without any preprocessing</span></h1></blockquote>","metadata":{}},{"cell_type":"code","source":"vocab = build_vocab(list(train_df['comment_text'].apply(lambda x:x.split())))\noov = check_coverage(vocab,glove_embeddings)\nfor i in oov[:10]:\n    print(COMMENT2+f\"{i}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-09T06:58:36.466023Z","iopub.execute_input":"2021-06-09T06:58:36.466293Z","iopub.status.idle":"2021-06-09T06:58:42.608512Z","shell.execute_reply.started":"2021-06-09T06:58:36.466265Z","shell.execute_reply":"2021-06-09T06:58:42.607409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote><h1 style=\"color: #484848;font-size: 20px;font-weight: bold;padding:30px;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\">Check coverage with standard preprocessing</span></h1></blockquote>","metadata":{}},{"cell_type":"code","source":"vocab = build_vocab(list(train_df['simple_preprocess'].apply(lambda x:x.split())))\noov = check_coverage(vocab,glove_embeddings)\nfor i in oov[:10]:\n    print(COMMENT1+f\"{i}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-08T22:18:40.340991Z","iopub.execute_input":"2021-06-08T22:18:40.341874Z","iopub.status.idle":"2021-06-08T22:18:43.191558Z","shell.execute_reply.started":"2021-06-08T22:18:40.341818Z","shell.execute_reply":"2021-06-08T22:18:43.185337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote><p style=\"color: #484848;font-size: 15px;font-weight: bold;padding:30px;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\">As we can see only 27.78% embeddings match to vocab of comments without any pre-processing and only 38.06% to comments with standard pre-processing.</span></p></blockquote>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"color: #484848;font-size: 20px;font-family: monospace; letter-spacing: 2px;cursor: pointer\"> Lets explore the embeddings, in particular symbols a bit. For that we first need to define \"what is a symbol\" in contrast to a regular letter.</p>","metadata":{"execution":{"iopub.status.busy":"2021-06-07T13:53:49.394517Z","iopub.execute_input":"2021-06-07T13:53:49.395044Z","iopub.status.idle":"2021-06-07T13:53:49.403581Z","shell.execute_reply.started":"2021-06-07T13:53:49.395004Z","shell.execute_reply":"2021-06-07T13:53:49.401861Z"}}},{"cell_type":"code","source":"latin_similar = \"‚Äô'‚Äò√Ü√ê∆é∆è∆ê∆îƒ≤≈ä≈í·∫û√û«∑»ú√¶√∞«ù…ô…õ…£ƒ≥≈ã≈ìƒ∏≈ø√ü√æ∆ø»ùƒÑ∆Å√áƒê∆äƒòƒ¶ƒÆ∆ò≈Å√ò∆†≈û»ò≈¢»ö≈¶≈≤∆ØYÃ®∆≥ƒÖ…ì√ßƒë…óƒôƒßƒØ∆ô≈Ç√∏∆°≈ü»ô≈£»õ≈ß≈≥∆∞yÃ®∆¥√Å√Ä√Ç√Ñ«çƒÇƒÄ√É√Ö«∫ƒÑ√Ü«º«¢∆ÅƒÜƒäƒàƒå√áƒé·∏åƒê∆ä√ê√â√àƒñ√ä√ãƒöƒîƒíƒò·∫∏∆é∆è∆êƒ†ƒú«¶ƒûƒ¢∆î√°√†√¢√§«éƒÉƒÅ√£√•«ªƒÖ√¶«Ω«£…ìƒáƒãƒâƒç√ßƒè·∏çƒë…ó√∞√©√®ƒó√™√´ƒõƒïƒìƒô·∫π«ù…ô…õƒ°ƒù«ßƒüƒ£…£ƒ§·∏§ƒ¶I√ç√åƒ∞√é√è«èƒ¨ƒ™ƒ®ƒÆ·ªäƒ≤ƒ¥ƒ∂∆òƒπƒª≈ÅƒΩƒø ºN≈ÉNÃà≈á√ë≈Ö≈ä√ì√í√î√ñ«ë≈é≈å√ï≈ê·ªå√ò«æ∆†≈íƒ•·∏•ƒßƒ±√≠√¨i√Æ√Ø«êƒ≠ƒ´ƒ©ƒØ·ªãƒ≥ƒµƒ∑∆ôƒ∏ƒ∫ƒº≈Çƒæ≈Ä≈â≈ÑnÃà≈à√±≈Ü≈ã√≥√≤√¥√∂«í≈è≈ç√µ≈ë·ªç√∏«ø∆°≈ì≈î≈ò≈ñ≈ö≈ú≈†≈û»ò·π¢·∫û≈§≈¢·π¨≈¶√û√ö√ô√õ√ú«ì≈¨≈™≈®≈∞≈Æ≈≤·ª§∆Ø·∫Ç·∫Ä≈¥·∫Ñ«∑√ù·ª≤≈∂≈∏»≤·ª∏∆≥≈π≈ª≈Ω·∫í≈ï≈ô≈ó≈ø≈õ≈ù≈°≈ü»ô·π£√ü≈•≈£·π≠≈ß√æ√∫√π√ª√º«î≈≠≈´≈©≈±≈Ø≈≥·ª•∆∞·∫É·∫Å≈µ·∫Ö∆ø√Ω·ª≥≈∑√ø»≥·ªπ∆¥≈∫≈º≈æ·∫ì\"\nwhite_list = string.ascii_letters + string.digits + latin_similar + ' '\nwhite_list += \"'\"","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-09T06:58:42.6103Z","iopub.execute_input":"2021-06-09T06:58:42.610605Z","iopub.status.idle":"2021-06-09T06:58:42.614991Z","shell.execute_reply.started":"2021-06-09T06:58:42.610573Z","shell.execute_reply":"2021-06-09T06:58:42.613964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote><p style=\"color: #484848;font-size: 15px;font-weight: bold;padding:30px;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\">Print all the symbols that we have an embeddings for.We can delete all symbols that we do not have embeddings.</span></p></blockquote>","metadata":{}},{"cell_type":"code","source":"glove_chars = ''.join([c for c in tqdm(glove_embeddings) if len(c) == 1])\nglove_symbols = ''.join([c for c in glove_chars if not c in white_list])\nglove_symbols","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-09T06:58:42.616653Z","iopub.execute_input":"2021-06-09T06:58:42.616989Z","iopub.status.idle":"2021-06-09T06:58:43.867397Z","shell.execute_reply.started":"2021-06-09T06:58:42.616952Z","shell.execute_reply":"2021-06-09T06:58:43.86638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote><p style=\"color: #484848;font-size: 15px;font-weight: bold;padding:30px;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\">Lets print all the symbols that are presented in our dataset.</span></p></blockquote>","metadata":{}},{"cell_type":"code","source":"jigsaw_chars = build_vocab(list(train_df[\"comment_text\"]))\njigsaw_symbols = ''.join([c for c in jigsaw_chars if not c in white_list])\njigsaw_symbols\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-09T06:58:43.86852Z","iopub.execute_input":"2021-06-09T06:58:43.868788Z","iopub.status.idle":"2021-06-09T06:58:50.691161Z","shell.execute_reply.started":"2021-06-09T06:58:43.86876Z","shell.execute_reply":"2021-06-09T06:58:50.690029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote><p style=\"color: #484848;font-size: 15px;font-weight: bold;padding:30px;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\">Print symbols that are presented in dataset but we do not have embeddings for.</span></p></blockquote>","metadata":{}},{"cell_type":"code","source":"symbols_to_delete = ''.join([c for c in jigsaw_symbols if not c in glove_symbols])\nsymbols_to_delete","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-09T06:58:50.692493Z","iopub.execute_input":"2021-06-09T06:58:50.692799Z","iopub.status.idle":"2021-06-09T06:58:50.70033Z","shell.execute_reply.started":"2021-06-09T06:58:50.692766Z","shell.execute_reply":"2021-06-09T06:58:50.69934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote><p style=\"color: #484848;font-size: 15px;font-weight: bold;padding:30px;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\">Print symbols that are presented in dataset and we also have embeddings for.</span></p></blockquote>","metadata":{}},{"cell_type":"code","source":"symbols_to_isolate = ''.join([c for c in jigsaw_symbols if c in glove_symbols])\nsymbols_to_isolate\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-09T06:58:50.701398Z","iopub.execute_input":"2021-06-09T06:58:50.701653Z","iopub.status.idle":"2021-06-09T06:58:50.714057Z","shell.execute_reply.started":"2021-06-09T06:58:50.701627Z","shell.execute_reply":"2021-06-09T06:58:50.713119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntqdm.pandas()\ntrain_df['comment_text'] = train_df['comment_text'].progress_apply(lambda x:remove_punctuation(x))\ntest_df['comment_text'] = test_df['comment_text'].progress_apply(lambda x:remove_punctuation(x))","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-09T06:58:50.716546Z","iopub.execute_input":"2021-06-09T06:58:50.71682Z","iopub.status.idle":"2021-06-09T06:58:58.056307Z","shell.execute_reply.started":"2021-06-09T06:58:50.716792Z","shell.execute_reply":"2021-06-09T06:58:58.05561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"color: #484848;font-size: 20px;font-family: monospace; letter-spacing: 2px;cursor: pointer\"> Now we build the function that will handle punctuation by removing all the symbols that we do not have embeddings for and isolating the symbols that we have embeddings for. </p>","metadata":{}},{"cell_type":"code","source":"isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\nremove_dict = {ord(c):f'' for c in symbols_to_delete}\n\n\ndef handle_punctuation(x):\n    x = x.translate(remove_dict)\n    x = x.translate(isolate_dict)\n    return x","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-09T06:58:58.059737Z","iopub.execute_input":"2021-06-09T06:58:58.060188Z","iopub.status.idle":"2021-06-09T06:58:58.066485Z","shell.execute_reply.started":"2021-06-09T06:58:58.060143Z","shell.execute_reply":"2021-06-09T06:58:58.065739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['comment_text'] = train_df['comment_text'].progress_apply(lambda x:handle_punctuation(x))\ntest_df['comment_text'] = test_df['comment_text'].progress_apply(lambda x:handle_punctuation(x))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-09T06:58:58.067749Z","iopub.execute_input":"2021-06-09T06:58:58.068333Z","iopub.status.idle":"2021-06-09T06:59:04.511692Z","shell.execute_reply.started":"2021-06-09T06:58:58.06829Z","shell.execute_reply":"2021-06-09T06:59:04.510926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote><p style=\"color: #484848;font-size: 15px;font-weight: bold;padding:30px;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\">Print result of fuction for handling punctuation and see coverage after removing symbols that we do not have embeddings for\n</span></p></blockquote>","metadata":{}},{"cell_type":"code","source":"vocab = build_vocab(list(train_df['comment_text'].apply(lambda x:x.split())))\noov = check_coverage(vocab,glove_embeddings)\nfor i in oov[:10]:\n    print(COMMENT2+f\"{i}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:59:04.512815Z","iopub.execute_input":"2021-06-09T06:59:04.513291Z","iopub.status.idle":"2021-06-09T06:59:10.082719Z","shell.execute_reply.started":"2021-06-09T06:59:04.513245Z","shell.execute_reply":"2021-06-09T06:59:10.081613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"color: #484848;font-size: 20px;font-family: monospace; letter-spacing: 2px;cursor: pointer\"> Now we can see in previus output data contains words with <span style=\"font-weight:bold\">appstrophe </span>that we need to remove because <span style=\"font-weight:bold\">isn't</span> and <span style=\"font-weight:bold\">is not</span>\nare different words for our word vector.</p>","metadata":{}},{"cell_type":"code","source":"train_df['comment_text'] = train_df['comment_text'].progress_apply(lambda x:remove_apostrophe(x))\ntest_df['comment_text'] = test_df['comment_text'].progress_apply(lambda x:remove_apostrophe(x))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-09T06:59:10.084903Z","iopub.execute_input":"2021-06-09T06:59:10.08551Z","iopub.status.idle":"2021-06-09T06:59:11.060385Z","shell.execute_reply.started":"2021-06-09T06:59:10.085461Z","shell.execute_reply":"2021-06-09T06:59:11.059299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = build_vocab(list(train_df['comment_text'].apply(lambda x:x.split())),verbose=False)\noov = check_coverage(vocab,glove_embeddings)\nfor i,j in zip(oov[:10],oov[10:20]):\n    print(COMMENT2+f\"{i}\")\n    print(COMMENT1+f\"{j}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:59:11.061873Z","iopub.execute_input":"2021-06-09T06:59:11.062452Z","iopub.status.idle":"2021-06-09T06:59:16.848895Z","shell.execute_reply.started":"2021-06-09T06:59:11.062404Z","shell.execute_reply":"2021-06-09T06:59:16.847629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_top_n_words(corpus, n=None):\n    tfidf = TfidfVectorizer()\n    tfidf = tfidf.fit(train_df['comment_text'])\n    bag_of_words = tfidf.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n    words_freq = [(word, sum_words[0,idx]) for word, idx in tfidf.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_n_bigram(corpus, n=None):\n    tfidf = TfidfVectorizer(ngram_range=(2,2))\n    tfidf = tfidf.fit(train_df['comment_text'])\n    bag_of_words = tfidf.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n    words_freq = [(word, sum_words[0,idx]) for word, idx in tfidf.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_n_trigram(corpus, n=None):\n    tfidf = TfidfVectorizer(ngram_range=(3,3))\n    tfidf = tfidf.fit(train_df['comment_text'])\n    bag_of_words = tfidf.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n    words_freq = [(word, sum_words[0,idx]) for word, idx in tfidf.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\n# functions for ploting the barchat\n\ndef plot_bt(fn,corpus,color,title):\n    common_words = fn(corpus,10)\n    common_words_df = DataFrame (common_words, columns=['word','freq'])\n    \n    colors =[]\n    for cols in color:\n        colors.extend([cols for i in range(2)])\n        \n    fig = go.Figure(go.Bar(\n        x = common_words_df['freq'],\n        y = common_words_df['word'],\n        marker_color = colors,\n        orientation='h',\n    ))\n    fig.update_traces(\n        marker_line_color = 'black',\n        marker_line_width = 1,\n        opacity=0.9,\n    )\n    fig.update_layout(\n        paper_bgcolor='#234E70',\n        plot_bgcolor='rgba(0,0,0,0)',\n        title=\"Top 10 \"+title[0]+\" of \"+title[1],\n        yaxis = {'categoryorder':'total ascending'},\n        xaxis = dict(side=\"top\", zerolinecolor = \"#4d4d4d\", zerolinewidth = 1, gridcolor=\"#e7e7e7\"),\n        font=dict(color='white',family=\"Courier New, monospace\"),\n    )\n    fig.show()\n    return common_words_df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-09T06:59:16.850555Z","iopub.execute_input":"2021-06-09T06:59:16.850995Z","iopub.status.idle":"2021-06-09T06:59:16.874229Z","shell.execute_reply.started":"2021-06-09T06:59:16.850948Z","shell.execute_reply":"2021-06-09T06:59:16.873406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_BuGn = px.colors.sequential.BuGn\ncolor_Blues = px.colors.sequential.Blues\ncolor_BuPu = px.colors.sequential.BuPu\ncolor_OrRd = px.colors.sequential.OrRd\ncolor_PuBu = px.colors.sequential.PuBu\ncolor_RdPu = px.colors.sequential.RdPu\ncolor_YlGn = px.colors.sequential.YlGn\ncolor_deep = px.colors.sequential.deep\ncolor_Tropic = px.colors.sequential.Purp\n\ncorpus_toxic = train_df['comment_text'][train_df.toxic==1]\ncorpus_severe_toxic = train_df['comment_text'][train_df.severe_toxic==1]\ncorpus_obscene = train_df['comment_text'][train_df.obscene==1]\ncorpus_threat = train_df['comment_text'][train_df.threat==1]\ncorpus_insult = train_df['comment_text'][train_df.insult==1]\ncorpus_identity_hate = train_df['comment_text'][train_df.identity_hate==1]\ncorpus_clean = train_df['comment_text'][train_df.none==1]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-09T06:59:16.875456Z","iopub.execute_input":"2021-06-09T06:59:16.875752Z","iopub.status.idle":"2021-06-09T06:59:16.907004Z","shell.execute_reply.started":"2021-06-09T06:59:16.875722Z","shell.execute_reply":"2021-06-09T06:59:16.905872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:cursive;\">\n    <h1 style=\"color: #484848;font-size: 35px;font-weight: bold;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\"><center>Unigram for all labels</center></span></h1>\n</div>","metadata":{}},{"cell_type":"code","source":"common_unigram_clean = plot_bt(get_top_n_words, corpus_clean,color_YlGn,[\"unigram\", \"clean comments\"])\ncommon_unigram_toxic = plot_bt(get_top_n_words,corpus_toxic,color_BuGn,[\"unigram\",\"toxic comments\"])\ncommon_unigram_severe_toxic = plot_bt(get_top_n_words,corpus_severe_toxic,color_Blues,[\"unigram\",\"severe toxic comments\"])\ncommon_unigram_obscene = plot_bt(get_top_n_words, corpus_obscene,color_BuPu, [\"unigram\",\"obscene comments\"])\ncommon_unigram_threat = plot_bt(get_top_n_words, corpus_threat,color_OrRd,[\"unigram\",\"threat comments\"])\ncommon_unigram_insult = plot_bt(get_top_n_words, corpus_insult, color_PuBu,[\"unigram\",\"insult comments\"])\ncommon_unigram_identity_hate = plot_bt(get_top_n_words,corpus_identity_hate,color_RdPu,[\"unigram\",\"identity hate comments\"])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-08T22:19:35.008723Z","iopub.execute_input":"2021-06-08T22:19:35.009167Z","iopub.status.idle":"2021-06-08T22:21:25.965026Z","shell.execute_reply.started":"2021-06-08T22:19:35.009122Z","shell.execute_reply":"2021-06-08T22:21:25.963583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:cursive;\">\n    <h1 style=\"color: #484848;font-size: 35px;font-weight: bold;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\"><center>Bigram for all labels</center></span></h1>\n</div>","metadata":{}},{"cell_type":"code","source":"common_bigram_clean = plot_bt(get_top_n_bigram, corpus_clean,color_YlGn,[\"bigram\", \"clean comments\"])\ncommon_bigram_toxic = plot_bt(get_top_n_bigram,corpus_toxic,color_BuGn,[\"bigram\",\"toxic comments\"])\ncommon_bigram_severe_toxic = plot_bt(get_top_n_bigram,corpus_severe_toxic,color_Blues,[\"bigram\",\"severe toxic comments\"])\ncommon_bigram_obscene = plot_bt(get_top_n_bigram, corpus_obscene,color_BuPu, [\"bigram\",\"obscene comments\"])\ncommon_bigram_threat = plot_bt(get_top_n_bigram, corpus_threat,color_OrRd,[\"bigram\",\"threat comments\"])\ncommon_bigram_insult = plot_bt(get_top_n_bigram, corpus_insult, color_PuBu,[\"bigram\",\"insult comments\"])\ncommon_bigram_identity_hate = plot_bt(get_top_n_bigram,corpus_identity_hate,color_RdPu,[\"bigram\",\"identity hate comments\"])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-08T22:21:37.529516Z","iopub.execute_input":"2021-06-08T22:21:37.529977Z","iopub.status.idle":"2021-06-08T22:26:30.058529Z","shell.execute_reply.started":"2021-06-08T22:21:37.529946Z","shell.execute_reply":"2021-06-08T22:26:30.057402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:cursive;\">\n    <h1 style=\"color: #484848;font-size: 35px;font-weight: bold;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\"><center>Trigram for all labels</center></span></h1>\n</div>","metadata":{}},{"cell_type":"code","source":"common_trigram_clean = plot_bt(get_top_n_trigram, corpus_clean,color_YlGn,[\"trigram\", \"clean comments\"])\ncommon_trigram_toxic = plot_bt(get_top_n_trigram,corpus_toxic,color_BuGn,[\"trigram\",\"toxic comments\"])\ncommon_trigram_severe_toxic = plot_bt(get_top_n_trigram,corpus_severe_toxic,color_Blues,[\"trigram\",\"severe toxic comments\"])\ncommon_trigram_obscene = plot_bt(get_top_n_trigram, corpus_obscene,color_BuPu, [\"trigram\",\"obscene comments\"])\ncommon_trigram_threat = plot_bt(get_top_n_trigram, corpus_threat,color_OrRd,[\"trigram\",\"threat comments\"])\ncommon_trigram_insult = plot_bt(get_top_n_trigram, corpus_insult, color_PuBu,[\"trigram\",\"insult comments\"])\ncommon_trigram_identity_hate = plot_bt(get_top_n_trigram,corpus_identity_hate,color_RdPu,[\"trigram\",\"identity hate comments\"])","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-08T22:27:06.559603Z","iopub.execute_input":"2021-06-08T22:27:06.559997Z","iopub.status.idle":"2021-06-08T22:35:51.287634Z","shell.execute_reply.started":"2021-06-08T22:27:06.559967Z","shell.execute_reply":"2021-06-08T22:35:51.286598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color: #484848;font-size: 20px;font-weight: bold;padding:30px;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\">Configuring TPU's</span></h1>","metadata":{}},{"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T06:59:16.908337Z","iopub.execute_input":"2021-06-09T06:59:16.908638Z","iopub.status.idle":"2021-06-09T06:59:22.587487Z","shell.execute_reply.started":"2021-06-09T06:59:16.908609Z","shell.execute_reply":"2021-06-09T06:59:22.586243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color: #484848;font-size: 20px;font-weight: bold;padding:30px;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\">Pre-process test data</span></h1>","metadata":{}},{"cell_type":"code","source":"def preprocess(x):\n    x = remove_punctuation(x)\n    x = handle_punctuation(x)\n    x = remove_apostrophe(x)\n    return x\n\ntest_df['comment_text'] = test_df['comment_text'].progress_apply(lambda x:preprocess(x))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-09T06:59:22.588794Z","iopub.execute_input":"2021-06-09T06:59:22.589124Z","iopub.status.idle":"2021-06-09T06:59:28.792689Z","shell.execute_reply.started":"2021-06-09T06:59:22.589091Z","shell.execute_reply":"2021-06-09T06:59:28.791937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x = train_df['comment_text']\ntest_x = test_df['comment_text']\ntrain_y = train_df[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-09T06:59:28.79396Z","iopub.execute_input":"2021-06-09T06:59:28.794429Z","iopub.status.idle":"2021-06-09T06:59:28.801679Z","shell.execute_reply.started":"2021-06-09T06:59:28.794396Z","shell.execute_reply":"2021-06-09T06:59:28.800669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color: #484848;font-size: 15px;font-weight: bold;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\">The below function is the most importent one yet,in this function we building a matrix for pre-trained embeddings and with that also match our words with embeddings for both upper and lower cases.</span></h1>","metadata":{}},{"cell_type":"code","source":"# This function is borrowed from \"https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda\"\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((max_features + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        if i <= max_features:\n            try:\n                embedding_matrix[i] = embedding_index[word]\n            except KeyError:\n                try:\n                    embedding_matrix[i] = embedding_index[word.lower()]\n                except KeyError:\n                    try:\n                        embedding_matrix[i] = embedding_index[word.title()]\n                    except KeyError:\n                        unknown_words.append(word)\n    return embedding_matrix, unknown_words\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-09T06:59:28.802927Z","iopub.execute_input":"2021-06-09T06:59:28.803377Z","iopub.status.idle":"2021-06-09T06:59:28.816555Z","shell.execute_reply.started":"2021-06-09T06:59:28.803343Z","shell.execute_reply":"2021-06-09T06:59:28.815731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color: #484848;font-size: 25px;font-weight: bold;padding:30px;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\">Tokenization</span></h1>\n<p style=\"color: #484848;font-size: 15px;font-weight: bold;padding:30px;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\">We need to perform tokenization - the processing of segmenting text into sentences of words. The benefit of tokenization is that it gets the text into a format that is easier to convert to raw numbers, which can actually be used for processing.</span></p>","metadata":{}},{"cell_type":"code","source":"max_features=400000\ntokenizer = Tokenizer(num_words=max_features,filters='',lower=False)\ntokenizer.fit_on_texts(list(train_x)+list(test_x))\n\ncrawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nglove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\nprint('n unknown words (glove): ', len(unknown_words_glove))\n\nmax_features = max_features or len(tokenizer.word_index) + 1\nmax_features\n\nembedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\nembedding_matrix.shape\n\ndel crawl_matrix\ndel glove_matrix\ngc.collect()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-09T06:59:28.817631Z","iopub.execute_input":"2021-06-09T06:59:28.81814Z","iopub.status.idle":"2021-06-09T07:00:52.028571Z","shell.execute_reply.started":"2021-06-09T06:59:28.818102Z","shell.execute_reply":"2021-06-09T07:00:52.0276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color: #484848;font-size: 25px;font-weight: bold;padding:30px;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\">\nSequence Bucketing</span></h1>","metadata":{"execution":{"iopub.status.busy":"2021-06-08T09:30:37.14369Z","iopub.execute_input":"2021-06-08T09:30:37.144141Z","iopub.status.idle":"2021-06-08T09:30:37.150891Z","shell.execute_reply.started":"2021-06-08T09:30:37.144043Z","shell.execute_reply":"2021-06-08T09:30:37.149282Z"}}},{"cell_type":"code","source":"train_x = tokenizer.texts_to_sequences(train_x)\ntest_x = tokenizer.texts_to_sequences(test_x)\n\nmaxlen = 300\nx_train_padded = pad_sequences(train_x, maxlen=maxlen)\nx_test_padded = pad_sequences(test_x, maxlen=maxlen)\nx_train_padded.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-09T07:00:52.030177Z","iopub.execute_input":"2021-06-09T07:00:52.030584Z","iopub.status.idle":"2021-06-09T07:01:14.153548Z","shell.execute_reply.started":"2021-06-09T07:00:52.03054Z","shell.execute_reply":"2021-06-09T07:01:14.15251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='7'></a>\n<h1 style=\"color: #484848;font-size: 35px;font-weight: bold;padding:30px;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\">TrainingüèãÔ∏è</span></h1>","metadata":{}},{"cell_type":"markdown","source":"<a id='7.1'></a>\n<h2 style=\"color: #484848;font-size: 25px;font-weight: bold;padding:30px;font-family: monospace; letter-spacing: 2px;cursor: pointer\" ><span style=\" transition: .5s linear\">Long Short Term Memory networks ‚Äì</span></h2>\n<a style=\"color: #484848;font-size: 15px;font-weight: bold;padding:30px;font-family: monospace; letter-spacing: 2px;cursor: pointer\" href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\"><span style=\" transition: .5s linear\">Long Short Term Memory networks ‚Äì usually just called ‚ÄúLSTMs‚Äù ‚Äì are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber (1997), and were refined and popularized by many people in following work. They work tremendously well on a large variety of problems, and are now widely used.</span></a>\n<br>\n<br>\n\n<p style=\"text-align:right;font-style:monospace;font-size:11px\">If you have no idea about what LSTM is i suggest you to click above text.</p>\n","metadata":{}},{"cell_type":"code","source":"%%time\nwith strategy.scope():\n    model = Sequential()\n    \n    model.add(Embedding(\n        input_dim=embedding_matrix.shape[0], \n        output_dim=embedding_matrix.shape[1], \n        weights = [embedding_matrix], \n        input_length=maxlen\n    ))\n    \n    model.add(Bidirectional(LSTM(\n        maxlen, \n        return_sequences = True, \n    )))\n    \n    model.add(GlobalMaxPool1D())\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(maxlen, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(maxlen, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(6, activation = 'sigmoid'))\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nmodel.summary()\n","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-09T07:06:29.066173Z","iopub.execute_input":"2021-06-09T07:06:29.066563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I will only run 2 epochs\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\n    'model.h5', \n    monitor = 'val_loss', \n    verbose = 1, \n    save_best_only = True\n)\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor = 'val_loss', \n    factor = 0.2, \n    verbose = 1, \n    patience = 1,                        \n    min_lr = 0.01\n)\nhistory = model.fit(\n    x_train_padded, \n    train_y, \n    epochs = 10,\n    batch_size = 512,\n    validation_split=0.25,\n    verbose = 1,\n    callbacks = [reduce_lr, checkpoint]\n )","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"load_model1 = load_model('./model.h5')","metadata":{"execution":{"iopub.status.busy":"2021-06-08T16:33:16.947865Z","iopub.execute_input":"2021-06-08T16:33:16.948312Z","iopub.status.idle":"2021-06-08T16:33:32.693449Z","shell.execute_reply.started":"2021-06-08T16:33:16.948264Z","shell.execute_reply":"2021-06-08T16:33:32.691671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction= load_model1.predict(x_test_padded)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T16:33:32.695114Z","iopub.execute_input":"2021-06-08T16:33:32.695467Z","iopub.status.idle":"2021-06-08T16:48:39.339718Z","shell.execute_reply.started":"2021-06-08T16:33:32.695436Z","shell.execute_reply":"2021-06-08T16:48:39.338688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unzip = zipfile.ZipFile('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip')\nunzip.extractall()\nsubm = pd.read_csv('./sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-08T16:48:39.34175Z","iopub.execute_input":"2021-06-08T16:48:39.342521Z","iopub.status.idle":"2021-06-08T16:48:39.663854Z","shell.execute_reply.started":"2021-06-08T16:48:39.342447Z","shell.execute_reply":"2021-06-08T16:48:39.662876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = prediction\n\nsubm.to_csv(\"sub_predictions8.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T16:48:39.665174Z","iopub.execute_input":"2021-06-08T16:48:39.665479Z","iopub.status.idle":"2021-06-08T16:48:42.344549Z","shell.execute_reply.started":"2021-06-08T16:48:39.665447Z","shell.execute_reply":"2021-06-08T16:48:42.34354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color: #484848;font-size: 35px;font-weight: bold;padding:30px;font-family: monospace; letter-spacing: 2px;cursor: pointer\"><span style=\" transition: .5s linear\">Refrences</span></h1>\n\n","metadata":{}},{"cell_type":"markdown","source":"<a style=\"font-size: 15px;font-weight: bold;padding:30px;font-family: monospace; letter-spacing: 2px;cursor: pointer\" href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">1.Understanding LSTM Neworks</a>\n\n<a style=\"font-size: 15px;font-weight: bold;padding:30px;font-family: monospace; letter-spacing: 2px;cursor: pointer\" href=\"https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda\">2.How To: Preprocessing for GloVe Part1: EDA</a>\n\n<a style=\"font-size: 15px;font-weight: bold;padding:30px;font-family: monospace; letter-spacing: 2px;cursor: pointer\" href=\"https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010\">3.Intuitive Guide to Understanding GloVe Embeddings</a>\n\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5642C5;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n        <p style=\"padding: 10px;\n              color:white;font-size:20px\">\n            Thanks for your time,And If you like this notebook please consider a upvote.\n        </p>\n    </div>","metadata":{}}]}